{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arbitrary-solid",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import copy\n",
    "import re\n",
    "import ast\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nibabel\n",
    "import scipy\n",
    "import scipy.io\n",
    "from pprint import pprint\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import _cov, LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA, FastICA\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "%matplotlib qt\n",
    "#matplotlib.use('Qt5Agg')\n",
    "\n",
    "import mne\n",
    "from mne.io import read_raw_fif, concatenate_raws\n",
    "from mne.viz import plot_alignment, set_3d_title\n",
    "from mne.preprocessing import ICA, create_eog_epochs, create_ecg_epochs\n",
    "from mne.decoding import SlidingEstimator, cross_val_multiscore, Scaler, Vectorizer\n",
    "from mne.decoding import UnsupervisedSpatialFilter\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D  # noqa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a63bf4-9d8c-424e-a153-4215c20cb0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set intial parameters\n",
    "\n",
    "tmin = -0.1\n",
    "tmax = 0.8\n",
    "baseline = (-0.1,0)\n",
    "\n",
    "downsample_freq = 200\n",
    "powerline_freq = (50, 100, 150, 200)\n",
    "low_pass_freq = 0.05\n",
    "high_pass_freq = 330\n",
    "\n",
    "# Savitzky-Golay filter\n",
    "window_length = 25\n",
    "polyorder = 4\n",
    "# Butterworth filter\n",
    "l_freq = 0.5\n",
    "h_freq = 40\n",
    "order = 2\n",
    "\n",
    "# Number of runs\n",
    "n_runs = 12\n",
    "\n",
    "# Subject\n",
    "subj = \"Subj42\"\n",
    "\n",
    "# Tail specify the hyperparameter (e.g. ica, autoreject, smoothing, ...)\n",
    "tail_fname = tail_fname = \"_savgol-\"+str(window_length)+\"-\"+str(polyorder)+\"_downsample-\"+str(downsample_freq)\n",
    "raw_fname = \"raw_tsss_trans.fif\"\n",
    "epoched_fname = \"epoched\"+tail_fname+\".mat\"\n",
    "log_fname = \"log\"+tail_fname+\".txt\"\n",
    "information_fname = \"information.mat\"\n",
    "\n",
    "data_folder = \"data\"\n",
    "subject_folder = \"subject\\\\\"+subj\n",
    "meg_folder = \"meg\"\n",
    "figure_folder = \"fig\"\n",
    "information_folder = \"information\"\n",
    "\n",
    "project_path = 'F:\\\\Data Fusion Project'\n",
    "\n",
    "raw_path = os.path.join(project_path,subject_folder,meg_folder)\n",
    "epoched_path = os.path.join(project_path,subject_folder,meg_folder)\n",
    "log_path = os.path.join(project_path,subject_folder,meg_folder)\n",
    "figure_path = os.path.join(project_path,subject_folder,meg_folder,figure_folder)\n",
    "information_path = os.path.join(project_path,data_folder,information_folder)\n",
    "\n",
    "# Events\n",
    "event_sample_list = ['N1S1TFA1', 'N1S1TFA2', 'N1S2TFA1', 'N1S2TFA2',\n",
    "                      'N1S3TFA1', 'N1S3TFA2', 'N1S4TFA1', 'N1S4TFA2',\n",
    "                      'N2S1TFA1', 'N2S1TFA2', 'N2S2TFA1', 'N2S2TFA2',\n",
    "                      'N2S3TFA1', 'N2S3TFA2', 'N2S4TFA1', 'N2S4TFA2',\n",
    "                      'N3S1TFA1', 'N3S1TFA2', 'N3S2TFA1', 'N3S2TFA2',\n",
    "                      'N3S3TFA1', 'N3S3TFA2', 'N3S4TFA1', 'N3S4TFA2',\n",
    "                      'N4S1TFA1', 'N4S1TFA2', 'N4S2TFA1', 'N4S2TFA2',\n",
    "                      'N4S3TFA1', 'N4S3TFA2', 'N4S4TFA1', 'N4S4TFA2']\n",
    "\n",
    "event_sample_dict = {'N1S1TFA1':1, 'N1S1TFA2':2, 'N1S2TFA1':3, 'N1S2TFA2':4,\n",
    "                     'N1S3TFA1':5, 'N1S3TFA2':6, 'N1S4TFA1':7, 'N1S4TFA2':8,\n",
    "                     'N2S1TFA1':9, 'N2S1TFA2':10, 'N2S2TFA1':11, 'N2S2TFA2':12,\n",
    "                     'N2S3TFA1':13, 'N2S3TFA2':14, 'N2S4TFA1':15, 'N2S4TFA2':16,\n",
    "                     'N3S1TFA1':17, 'N3S1TFA2':18, 'N3S2TFA1':19, 'N3S2TFA2':20,\n",
    "                     'N3S3TFA1':21, 'N3S3TFA2':22, 'N3S4TFA1':23, 'N3S4TFA2':24,\n",
    "                     'N4S1TFA1':25, 'N4S1TFA2':26, 'N4S2TFA1':27, 'N4S2TFA2':28,\n",
    "                     'N4S3TFA1':29, 'N4S3TFA2':30, 'N4S4TFA1':31, 'N4S4TFA2':32}\n",
    "\n",
    "event_sample_dict_N = {'N1/N1S1TFA1':1, 'N1/N1S1TFA2':2, 'N1/N1S2TFA1':3, 'N1/N1S2TFA2':4,\n",
    "                      'N1/N1S3TFA1':5, 'N1/N1S3TFA2':6, 'N1/N1S4TFA1':7, 'N1/N1S4TFA2':8,\n",
    "                      'N2/N2S1TFA1':9, 'N2/N2S1TFA2':10, 'N2/N2S2TFA1':11, 'N2/N2S2TFA2':12,\n",
    "                      'N2/N2S3TFA1':13, 'N2/N2S3TFA2':14, 'N2/N2S4TFA1':15, 'N2/N2S4TFA2':16,\n",
    "                      'N3/N3S1TFA1':17, 'N3/N3S1TFA2':18, 'N3/N3S2TFA1':19, 'N3/N3S2TFA2':20,\n",
    "                      'N3/N3S3TFA1':21, 'N3/N3S3TFA2':22, 'N3/N3S4TFA1':23, 'N3/N3S4TFA2':24,\n",
    "                      'N4/N4S1TFA1':25, 'N4/N4S1TFA2':26, 'N4/N4S2TFA1':27, 'N4/N4S2TFA2':28,\n",
    "                      'N4/N4S3TFA1':29, 'N4/N4S3TFA2':30, 'N4/N4S4TFA1':31, 'N4/N4S4TFA2':32}\n",
    "\n",
    "event_sample_dict_TFA = {'TFA1/N1S1TFA1':1, 'TFA2/N1S1TFA2':2, 'TFA1/N1S2TFA1':3, 'TFA2/N1S2TFA2':4,\n",
    "                        'TFA1/N1S3TFA1':5, 'TFA2/N1S3TFA2':6, 'TFA1/N1S4TFA1':7, 'TFA2/N1S4TFA2':8,\n",
    "                        'TFA1/N2S1TFA1':9, 'TFA2/N2S1TFA2':10, 'TFA1/N2S2TFA1':11, 'TFA2/N2S2TFA2':12,\n",
    "                        'TFA1/N2S3TFA1':13, 'TFA2/N2S3TFA2':14, 'TFA1/N2S4TFA1':15, 'TFA2/N2S4TFA2':16,\n",
    "                        'TFA1/N3S1TFA1':17, 'TFA2/N3S1TFA2':18, 'TFA1/N3S2TFA1':19, 'TFA2/N3S2TFA2':20,\n",
    "                        'TFA1/N3S3TFA1':21, 'TFA2/N3S3TFA2':22, 'TFA1/N3S4TFA1':23, 'TFA2/N3S4TFA2':24,\n",
    "                        'TFA1/N4S1TFA1':25, 'TFA2/N4S1TFA2':26, 'TFA1/N4S2TFA1':27, 'TFA2/N4S2TFA2':28,\n",
    "                        'TFA1/N4S3TFA1':29, 'TFA2/N4S3TFA2':30, 'TFA1/N4S4TFA1':31, 'TFA2/N4S4TFA2':32}\n",
    "\n",
    "event_sample_dict_S = {'S1/N1S1TFA1':1, 'S1/N1S1TFA2':2, 'S2/N1S2TFA1':3, 'S2/N1S2TFA2':4,\n",
    "                      'S3/N1S3TFA1':5, 'S3/N1S3TFA2':6, 'S4/N1S4TFA1':7, 'S4/N1S4TFA2':8,\n",
    "                      'S1/N2S1TFA1':9, 'S1/N2S1TFA2':10, 'S2/N2S2TFA1':11, 'S2/N2S2TFA2':12,\n",
    "                      'S3/N2S3TFA1':13, 'S3/N2S3TFA2':14, 'S4/N2S4TFA1':15, 'S4/N2S4TFA2':16,\n",
    "                      'S1/N3S1TFA1':17, 'S1/N3S1TFA2':18, 'S2/N3S2TFA1':19, 'S2/N3S2TFA2':20,\n",
    "                      'S3/N3S3TFA1':21, 'S3/N3S3TFA2':22, 'S4/N3S4TFA1':23, 'S4/N3S4TFA2':24,\n",
    "                      'S1/N4S1TFA1':25, 'S1/N4S1TFA2':26, 'S2/N4S2TFA1':27, 'S2/N4S2TFA2':28,\n",
    "                      'S3/N4S3TFA1':29, 'S3/N4S3TFA2':30, 'S4/N4S4TFA1':31, 'S4/N4S4TFA2':32}\n",
    "\n",
    "event_dict_N = {'N1':1, 'N2':2, 'N3':3, 'N4':4, 'Smaller Match':5, 'Larger Match':6, 'Left Button':7, 'Right Button':8}\n",
    "\n",
    "event_dict_S = {'S1':1, 'S2':2, 'S3':3, 'S4':4, 'Smaller Match':5, 'Larger Match':6, 'Left Button':7, 'Right Button':8}\n",
    "\n",
    "event_dict_TFA = {'TFA1':1, 'TFA2':2, 'Smaller Match':3, 'Larger Match':4, 'Left Button':5, 'Right Button':6}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a48e53-6dc0-4022-a324-fe2ddd1545ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_multi_image(figs,fname):\n",
    "    pdf = PdfPages(fname)\n",
    "    for fig in range(len(figs)):\n",
    "        pdf.savefig(figs[fig])\n",
    "    pdf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appropriate-translator",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare files\n",
    "# Rename fif files and make fig folder inside meg folder\n",
    "\n",
    "def prepare_file(rename_file):\n",
    "    if rename_file:\n",
    "        os.chdir(raw_path)\n",
    "        path = glob.glob(\"*.fif\")\n",
    "\n",
    "        regex = re.compile(r'\\d+')\n",
    "\n",
    "        for fn in path:\n",
    "            inx = regex.findall(fn)[3]\n",
    "            os.rename(fn,inx.lstrip('0')+\"_\"+raw_fname)\n",
    "            \n",
    "    if not os.path.exists(figure_path):\n",
    "        os.makedirs(figure_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "julian-congress",
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_data():\n",
    "    path = os.path.join(raw_path,\"%i_\"+raw_fname)\n",
    "    raws = [read_raw_fif(path % run, verbose='error')\n",
    "            for run in range(1,n_runs+1)]  # ignore filename warnings\n",
    "    \n",
    "    return raws\n",
    "\n",
    "#--------------------------------------------------#\n",
    "\n",
    "def import_info():\n",
    "    def loadmat(filename):\n",
    "        '''\n",
    "        this function should be called instead of direct spio.loadmat\n",
    "        as it cures the problem of not properly recovering python dictionaries\n",
    "        from mat files. It calls the function check keys to cure all entries\n",
    "        which are still mat-objects\n",
    "        '''\n",
    "        def _check_keys(d):\n",
    "            '''\n",
    "            checks if entries in dictionary are mat-objects. If yes\n",
    "            todict is called to change them to nested dictionaries\n",
    "            '''\n",
    "            for key in d:\n",
    "                if isinstance(d[key], scipy.io.matlab.mio5_params.mat_struct):\n",
    "                    d[key] = _todict(d[key])\n",
    "            return d\n",
    "\n",
    "        def _todict(matobj):\n",
    "            '''\n",
    "            A recursive function which constructs from matobjects nested dictionaries\n",
    "            '''\n",
    "            d = {}\n",
    "            for strg in matobj._fieldnames:\n",
    "                elem = matobj.__dict__[strg]\n",
    "                if isinstance(elem, scipy.io.matlab.mio5_params.mat_struct):\n",
    "                    d[strg] = _todict(elem)\n",
    "                elif isinstance(elem, np.ndarray):\n",
    "                    d[strg] = _tolist(elem)\n",
    "                else:\n",
    "                    d[strg] = elem\n",
    "            return d\n",
    "\n",
    "        def _tolist(ndarray):\n",
    "            '''\n",
    "            A recursive function which constructs lists from cellarrays\n",
    "            (which are loaded as numpy ndarrays), recursing into the elements\n",
    "            if they contain matobjects.\n",
    "            '''\n",
    "            elem_list = []\n",
    "            for sub_elem in ndarray:\n",
    "                if isinstance(sub_elem, scipy.io.matlab.mio5_params.mat_struct):\n",
    "                    elem_list.append(_todict(sub_elem))\n",
    "                elif isinstance(sub_elem, np.ndarray):\n",
    "                    elem_list.append(_tolist(sub_elem))\n",
    "                else:\n",
    "                    elem_list.append(sub_elem)\n",
    "            return elem_list\n",
    "        data = scipy.io.loadmat(filename, struct_as_record=False, squeeze_me=True)\n",
    "        return _check_keys(data)\n",
    "    \n",
    "    path = os.path.join(information_path,information_fname)\n",
    "    subject_info = loadmat(path)\n",
    "    \n",
    "    return subject_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5bfe8a-e854-4c5a-aaba-da1965966ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_raw_obj(raws, on_mismatch='raise'):\n",
    "    \n",
    "    from mne.io.meas_info import _ensure_infos_match\n",
    "    \n",
    "    # Make a deepcopy of raws to keep raws intact\n",
    "    #https://robertheaton.com/2014/02/09/pythons-pass-by-object-reference-as-explained-by-philip-k-dick/\n",
    "    _raws = copy.deepcopy(raws)\n",
    "    \n",
    "    for idx, r in enumerate(_raws[1:], start=1):\n",
    "        _ensure_infos_match(info1=_raws[0].info, info2=r.info, name=f'raws[{idx}]', on_mismatch=on_mismatch)\n",
    "        \n",
    "    _raws[0].append(_raws[1:])\n",
    "    \n",
    "    return _raws[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1daf757-c3db-4765-b53c-0e3eba5ab6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_raw_obj(raws, subject_info):\n",
    "    for r in range(1,n_runs+1):\n",
    "        drop_channels = subject_info['meg'][subj]['channels']['drop']['run'+str(r)]\n",
    "        rename_channels = subject_info['meg'][subj]['channels']['rename']['run'+str(r)]\n",
    "\n",
    "        if rename_channels != 'none':\n",
    "            channel_dict = ast.literal_eval(rename_channels)\n",
    "            for c in channel_dict.items():\n",
    "                try:\n",
    "                    raws[r-1].rename_channels(dict([c]))\n",
    "                except:\n",
    "                    raws[r-1].drop_channels(list(dict([c]).values()))\n",
    "                    raws[r-1].rename_channels(dict([c]))\n",
    "        if drop_channels != 'none':\n",
    "            raws[r-1].drop_channels(drop_channels)\n",
    "\n",
    "        if not 'EOG061' in subject_info['meg'][subj]['channels']['drop']['run'+str(r)]:\n",
    "            raws[r-1].set_channel_types({'EOG061':'eog'})\n",
    "        if not 'EOG062' in subject_info['meg'][subj]['channels']['drop']['run'+str(r)]:\n",
    "            raws[r-1].set_channel_types({'EOG062':'eog'})\n",
    "        if not 'ECG063' in subject_info['meg'][subj]['channels']['drop']['run'+str(r)]:\n",
    "            raws[r-1].set_channel_types({'ECG063':'ecg'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strategic-coalition",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_info(raw):\n",
    "    print(raw.info)\n",
    "    #print()   # insert a blank line in the output\n",
    "    #print('Bad channels:', raw.info['bads'])\n",
    "    #print('Sampling frequency:', raw.info['sfreq'], 'Hz')\n",
    "\n",
    "    #raw.info['chs']\n",
    "    #raw.info['dig']\n",
    "\n",
    "    #print(raw.info['bads'])\n",
    "    #print(raw.info['ch_names'])\n",
    "\n",
    "    #info = mne.io.read_info(raw)\n",
    "    #print(info.keys())\n",
    "    #print(info['ch_names'])\n",
    "    #print(info['bads'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secret-absorption",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vis_plot_raw(raw):\n",
    "    raw.plot(duration=5, n_channels=20, group_by='position')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "individual-dancing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_bad_channels(raw):\n",
    "    raw.info['bads']\n",
    "\n",
    "    raw.plot_psd(fmax=50, picks=['meg'])\n",
    "    raw.plot_psd_topo(fmax=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numerous-canberra",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_diode(raw):\n",
    "    \"\"\"\n",
    "    Correct event timing using photodiode, get the individual directions timings and calculate\n",
    "    the delay due to stimuli presentation\n",
    "\n",
    "    adapted from cimec_correct_diode\n",
    "    \"\"\"\n",
    "\n",
    "    raw_ = raw.copy()\n",
    "    \n",
    "    # get the photodiode data\n",
    "    diode_data = raw_.get_data(picks=raw_.ch_names.index('MISC008')).squeeze()\n",
    "    # low pass at 15 hz, this makes the correction robust to diode displacements\n",
    "    diode_data = mne.filter.filter_data(diode_data, raw_.info['sfreq'], l_freq=None, h_freq=15)\n",
    "    # initialize samples indices\n",
    "    samples = np.arange(0, len(diode_data))\n",
    "    # diode goes from gray to black, then continues black for the trajectories\n",
    "    # here we invert the values so that the first change is positive\n",
    "    diode_data = np.abs(diode_data - diode_data.max())\n",
    "    # subject specific \"baseline noise\", to account for diodes displacement\n",
    "    diode_min, diode_max = np.amin(diode_data[10:500]), np.amax(diode_data[10:500])\n",
    "    # create a strict binary mask: 1 == stimulus present\n",
    "    mask = diode_data >= diode_max\n",
    "    # find out when it changes to get the onset\n",
    "    onsets = np.nonzero(np.diff(mask) > 0)[0]\n",
    "    # find events from triggers\n",
    "    events_ = mne.find_events(raw_, stim_channel='STI101', min_duration=.005, shortest_event=1, output='onset')\n",
    "    # keep only the events coded also using the diode\n",
    "    # Right/Left button keys pressed\n",
    "    exclude_events = [128+1,128+2,128+4,128+8]\n",
    "    start_trg = mne.pick_events(events_, exclude=exclude_events)\n",
    "    # initialize the output\n",
    "    start_correct = start_trg.copy()\n",
    "    trj_list = []\n",
    "    # initialize the figure\n",
    "    #fig, ax = plt.subplots(len(start_trg), 1, dpi=150, sharey=True, figsize=(15, 8))\n",
    "    # then correct each event\n",
    "    for i_event, start_event in enumerate(start_trg):\n",
    "        # 1) find trial start\n",
    "        # get the onset of the trial according to meg trigger\n",
    "        meg_trg_start = start_event[0] - raw_.first_samp\n",
    "        # define trial samples\n",
    "        trl_ind = np.arange(meg_trg_start - 100, meg_trg_start + 0.5 * 1000)\n",
    "        # now find the onsets that are:\n",
    "        # - after the start according to the meg trigger\n",
    "        # - within the tolerance value (in samples)\n",
    "        start_ind = np.nonzero(np.logical_and(onsets - meg_trg_start >= 0,\n",
    "                                              (onsets - meg_trg_start) <= 150))[0]\n",
    "        start_correct[i_event, 0] = onsets[min(start_ind)]\n",
    "        \n",
    "        \"\"\"\n",
    "        # plot\n",
    "        ax[i_event].plot(samples[trl_ind], diode_data[trl_ind])\n",
    "        ax[i_event].vlines(samples[meg_trg_start], diode_data.min(), diode_data.max(), 'r', label='meg trigger')\n",
    "        ax[i_event].vlines(samples[start_correct[i_event, 0]], diode_data.min(), diode_data.max(), 'g', label='corrected')\n",
    "        ax[i_event].set_xlim(meg_trg_start - 50, meg_trg_start + 500)\n",
    "        \"\"\"\n",
    "        \n",
    "    # add back the first sample\n",
    "    start_correct[:, 0] += raw_.first_samp\n",
    "    \n",
    "    \"\"\"\n",
    "    # display corrected timings\n",
    "    delta_t = (start_correct[:, 0] - start_trg[:, 0]) / raw_.info['sfreq']\n",
    "    logging.getLogger('mne').info(f\"Photodiode correction: +{np.mean(delta_t):2.2f} Â± {np.std(delta_t):2.2f} s\")\n",
    "    plt.suptitle(f\"photodiode correction: +{np.mean(delta_t):2.2f}Â±{np.std(delta_t):2.2f} s\")\n",
    "    ax[0].legend()\n",
    "    ax[-1].set_xlabel('Time (samples)')\n",
    "    plt.setp(ax, xticks=[], yticks=[])\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(os.path.join(figure_path,diode), dpi=300)\n",
    "    \"\"\"\n",
    "    \n",
    "    return start_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "valid-parcel",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_annot_interactive(raw):\n",
    "    fig = raw.plot(duration=5, n_channels=20, group_by='position')\n",
    "    fig.canvas.key_press_event('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bibliographic-northern",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_annot_eog(raw):\n",
    "    eog_events = mne.preprocessing.find_eog_events(raw)\n",
    "    eog_onsets = eog_events[:,0] / raw.info['sfreq'] - 0.1\n",
    "    eog_durations = [0.3] * len(eog_events)\n",
    "    eog_descriptions = ['BAD_EOG'] * len(eog_events)\n",
    "    \n",
    "    eog_annot = mne.Annotations(eog_onsets,eog_durations,eog_descriptions,orig_time=raw.info['meas_date'])\n",
    "\n",
    "    raw.set_annotations(eog_annot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "placed-accessory",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_annot_ecg(raw):\n",
    "    ecg_events = mne.preprocessing.find_ecg_events(raw)[0]\n",
    "    ecg_onsets = ecg_events[:, 0] / raw.info['sfreq'] - 0.05\n",
    "    ecg_durations = [0.15] * len(ecg_events)\n",
    "    ecg_descriptions = ['BAD_ECG'] * len(ecg_events)\n",
    "    \n",
    "    ecg_annot = mne.Annotations(ecg_onsets,ecg_durations,ecg_descriptions,orig_time=raw.info['meas_date'])\n",
    "\n",
    "    raw.set_annotations(ecg_annot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parliamentary-oregon",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_annot_muscle(raw):\n",
    "    _raw = raw.copy()\n",
    "    _raw.load_data()\n",
    "\n",
    "    # The threshold is data dependent, check the optimal threshold by plotting muscle_scores.\n",
    "    muscle_threshold = 5  # z-score\n",
    "    # Choose one channel type, if there are axial gradiometers and magnetometers\n",
    "    # Select magnetometers as they are more sensitive to muscle activity\n",
    "    muscle_annot, muscle_scores = mne.preprocessing.annotate_muscle_zscore(_raw, ch_type=\"mag\", threshold=muscle_threshold, min_length_good=0.2, filter_freq=[110, 140])\n",
    "\n",
    "    _raw.set_annotations(muscle_annot)\n",
    "\n",
    "    # Plot muscle z-scores across recording\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(_raw.times, muscle_scores)\n",
    "    ax.axhline(y=muscle_threshold, color='r')\n",
    "    ax.set(xlabel='time, (s)', ylabel='zscore', title='Muscle activity')\n",
    "    \n",
    "    fname = str(r+1)+'_muscle'+tail_fname+'.png'\n",
    "    fig.savefig(os.path.join(figure_path,fname))\n",
    "    matplotlib.pyplot.close('all')\n",
    "    \n",
    "    return _raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "federal-colors",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_power_noise(raw,r):\n",
    "    \"\"\"\n",
    "    def add_arrows(axes):\n",
    "        # add some arrows at 50 Hz and its harmonics\n",
    "        for ax in axes:\n",
    "            freqs = ax.lines[-1].get_xdata()\n",
    "            psds = ax.lines[-1].get_ydata()\n",
    "            for freq in freqs:\n",
    "                idx = np.searchsorted(freqs, freq)\n",
    "                # get ymax of a small region around the freq. of interest\n",
    "                y = psds[(idx - 4):(idx + 5)].max()\n",
    "                ax.arrow(x=freqs[idx], y=y + 18, dx=0, dy=-12, color='red',\n",
    "                         width=0.1, head_width=3, length_includes_head=True)\n",
    "    \"\"\"\n",
    "\n",
    "    _raw = raw.copy()\n",
    "    _raw.load_data()\n",
    "\n",
    "    meg_picks = mne.pick_types(_raw.info, meg=True)\n",
    "    freqs = powerline_freq\n",
    "    _raw.notch_filter(freqs=freqs, picks=meg_picks)\n",
    "\n",
    "    figs = []\n",
    "    for title, data in zip(['Un', 'Notch '], [raw, _raw]):\n",
    "        fig = data.plot_psd(fmax=250, average=True)\n",
    "        fig.subplots_adjust(top=0.85)\n",
    "        fig.suptitle('{}filtered'.format(title), size='xx-large', weight='bold')\n",
    "        #add_arrows(fig.axes[:2])\n",
    "        figs.append(fig)\n",
    "    \n",
    "    fname = str(r+1)+'_filter_powerline'+tail_fname+'.pdf'\n",
    "    save_multi_image(figs,os.path.join(figure_path,fname))\n",
    "    matplotlib.pyplot.close('all')\n",
    "    \n",
    "    return _raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "changing-transfer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_band(raw,r):\n",
    "    _raw = raw.copy()\n",
    "    _raw.load_data()\n",
    "\n",
    "    meg_picks = mne.pick_types(_raw.info, meg=True)\n",
    "    _raw.filter(low_pass_freq, high_pass_freq, fir_design='firwin', picks=meg_picks)\n",
    "\n",
    "    figs = []\n",
    "    for title, data in zip(['un', ''], [raw, _raw]):\n",
    "        fig = data.plot_psd(fmax=250, average=True)\n",
    "        fig.subplots_adjust(top=0.85)\n",
    "        fig.suptitle('{}filtered'.format(title), size='xx-large', weight='bold')\n",
    "        fname = 'filter_band_'+'r'+str(r+1)+'_'+'{}filtered'.format(title)+tail_fname+'.png'\n",
    "        figs.append(fig)\n",
    "    \n",
    "    fname = str(r+1)+'_filter_bandpass'+tail_fname+'.pdf'\n",
    "    save_multi_image(figs,os.path.join(figure_path,fname))\n",
    "    matplotlib.pyplot.close('all')\n",
    "\n",
    "    return _raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a7e2e7-6303-4476-9893-86e1c6dee093",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_savgol_raw(raw):\n",
    "    #Savitzky-Golay filter\n",
    "    def savgol(x):\n",
    "        return scipy.signal.savgol_filter(x,window_length,polyorder)\n",
    "\n",
    "    _raw = raw.copy()\n",
    "    _raw.load_data()\n",
    "\n",
    "    meg_picks = mne.pick_types(_raw.info, meg=True)\n",
    "    _raw.apply_function(savgol, picks=meg_picks, dtype=None, n_jobs=1, channel_wise=True)\n",
    "\n",
    "    return _raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d895f375-0805-4919-88bc-f4920800ec27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_butter_raw(raw):\n",
    "    _raw = raw.copy()\n",
    "    _raw.load_data()\n",
    "\n",
    "    iir_params = dict(order=order, ftype='butter')\n",
    "    meg_picks = mne.pick_types(_raw.info, meg=True)\n",
    "    _raw.filter(l_freq=l_freq, h_freq=h_freq, picks=meg_picks, method='iir', iir_params=iir_params, verbose=True)\n",
    "\n",
    "    return _raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "velvet-regular",
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsample_raw(raw):\n",
    "    #raw.resample(120,npad=\"auto\") # set sampling frequency to 120Hz\n",
    "\n",
    "    current_sfreq = raw.info['sfreq']\n",
    "    desired_sfreq = downsample_freq\n",
    "    decim = np.round(current_sfreq / desired_sfreq).astype(int)\n",
    "    obtained_sfreq = current_sfreq / decim\n",
    "    lowpass_freq = obtained_sfreq / 3.\n",
    "\n",
    "    _raw = raw.copy()\n",
    "    _raw.load_data()\n",
    "\n",
    "    _raw.filter(l_freq=None, h_freq=lowpass_freq)\n",
    "\n",
    "    return _raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rolled-watson",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_ica(raw,r):\n",
    "    figs = []\n",
    "\n",
    "    # Filtering to remove slow drifts\n",
    "    # As filtering is a linear operation, the ICA solution found from the filtered signal can be applied to the unfiltered signal\n",
    "    _raw = raw.copy()\n",
    "    _raw.load_data().filter(l_freq=1., h_freq=None)\n",
    "\n",
    "    # Estimate noise covariance matrix from a continuous segment of raw data.\n",
    "    # It is typically useful to estimate a noise covariance from empty room data or time intervals before starting the stimulation.\n",
    "    #noise_cov = mne.compute_raw_covariance(raw_erm, tmin=0, tmax=None)\n",
    "\n",
    "    # Fitting the ICA solution\n",
    "    ica = ICA(n_components=30, max_iter=1000, random_state=97, method='picard')\n",
    "    #ica = ICA(n_components=15, max_iter=1000, random_state=97, method='picard', noise_cov=noise_cov)\n",
    "    ica.fit(_raw)\n",
    "    \n",
    "    figs.append(ica.plot_sources(raw, show_scrollbars=False))\n",
    "    figs.append(ica.plot_components(picks=slice(0,30,1)))\n",
    "\n",
    "    ecg_idx = mne.pick_types(raw.info, meg=False, eeg=False, stim=False,\n",
    "                             eog=False, ecg=True, emg=False, ref_meg=False,\n",
    "                             exclude='bads')\n",
    "    \n",
    "    eog_idx = mne.pick_types(raw.info, meg=False, eeg=False, stim=False,\n",
    "                             eog=True, ecg=False, emg=False, ref_meg=False,\n",
    "                             exclude='bads')\n",
    "    \n",
    "    if list(ecg_idx)!=[]:\n",
    "        ecg_evoked = create_ecg_epochs(raw).average()\n",
    "        ecg_evoked.apply_baseline(baseline=(None, -0.2))\n",
    "        ecg_indices, ecg_scores = ica.find_bads_ecg(raw, method='correlation', threshold='auto')\n",
    "        ecgAvailable = True\n",
    "    else:\n",
    "        ecgAvailable = False\n",
    "    \n",
    "    if eog_idx!=[]:\n",
    "        eog_evoked = create_eog_epochs(raw).average()\n",
    "        eog_evoked.apply_baseline(baseline=(None, -0.2))\n",
    "        eog_indices, eog_scores = ica.find_bads_eog(raw)\n",
    "        eogAvailable = True\n",
    "    else:\n",
    "        eogAvailable = False\n",
    "\n",
    "    # Save figures temporarily for inspection\n",
    "    fname=str(r+1)+'_temp_figs.pdf'\n",
    "    save_multi_image(figs,os.path.join(figure_path,fname))\n",
    "    matplotlib.pyplot.close('all')\n",
    "        \n",
    "    # If EOG channels are not available, user should select the components manually\n",
    "    if not eogAvailable:\n",
    "        eog_indices = []\n",
    "        print(\"EOG was not available, consequently, please select the EOG components manually!\")\n",
    "        n = int(input(\"Please enter the number of EOG component(s) you want to exclude (RUN \"+str(r+1) +\"): \"))\n",
    "        for i in range(n):\n",
    "            eog_indices.append(int(input(\"Please enter the index of the  EOG component(s) you want to exclude one by one: \")))\n",
    "            \n",
    "    # If ECG channel is not available, user should select the components manually\n",
    "    if not ecgAvailable:\n",
    "        ecg_indices = []\n",
    "        print(\"ECG was not available, consequently, please select the ECG components manually!\")\n",
    "        n = int(input(\"Please enter the number of ECG component(s) you want to exclude (RUN \"+str(r+1) +\"): \"))\n",
    "        for i in range(n):\n",
    "            ecg_indices.append(int(input(\"Please enter the index of the ECG component(s) you want to exclude one by one: \")))\n",
    "            \n",
    "    # Delete temporary figures\n",
    "    os.remove(os.path.join(figure_path,fname))\n",
    "\n",
    "    if eogAvailable and eog_indices!=[]:\n",
    "        ica.exclude = []\n",
    "        ica.exclude = eog_indices\n",
    "\n",
    "        # Barplot of ICA component \"EOG match\" scores\n",
    "        figs.append(ica.plot_scores(eog_scores))\n",
    "\n",
    "        # Plot diagnostics\n",
    "        figs = figs + ica.plot_properties(raw, picks=eog_indices)\n",
    "\n",
    "        # Plot ICs applied to raw data, with EOG matches highlighted\n",
    "        figs.append(ica.plot_sources(raw, show_scrollbars=False))\n",
    "\n",
    "        # Plot ICs applied to the averaged EOG epochs, with EOG matches highlighted\n",
    "        figs.append(ica.plot_sources(eog_evoked))\n",
    "\n",
    "        ica.exclude = []\n",
    "        try:\n",
    "            eog_index = [eog_indices[0]]\n",
    "        except:\n",
    "            eog_index = [eog_indices]\n",
    "    elif eog_indices==[]:\n",
    "        eog_index = []\n",
    "    else:\n",
    "        try:\n",
    "            eog_index = [eog_indices[0]]\n",
    "        except:\n",
    "            eog_index = [eog_indices]\n",
    "\n",
    "    if ecgAvailable and ecg_indices!=[]:\n",
    "        ica.exclude = []\n",
    "        ica.exclude = ecg_indices\n",
    "\n",
    "        # Barplot of ICA component \"ECG match\" scores\n",
    "        figs.append(ica.plot_scores(ecg_scores))\n",
    "\n",
    "        # Plot diagnostics\n",
    "        figs = figs + ica.plot_properties(raw, picks=ecg_indices)\n",
    "\n",
    "        # Plot ICs applied to raw data, with ECG matches highlighted\n",
    "        figs.append(ica.plot_sources(raw, show_scrollbars=False))\n",
    "\n",
    "        # Plot ICs applied to the averaged ECG epochs, with ECG matches highlighted\n",
    "        figs.append(ica.plot_sources(ecg_evoked))\n",
    "\n",
    "        ica.exclude = []\n",
    "        try:\n",
    "            ecg_index = [ecg_indices[0]]\n",
    "        except:\n",
    "            ecg_index = [ecg_indices]\n",
    "    elif ecg_indices==[]:\n",
    "        ecg_index = []\n",
    "    else:\n",
    "        try:\n",
    "            ecg_index = [ecg_indices[0]]\n",
    "        except:\n",
    "            ecg_index = [ecg_indices]\n",
    "\n",
    "    # Exclude only the first components\n",
    "    ica.exclude = eog_index+ecg_index\n",
    "    print(\"EOG index \"+str(eog_index)+\" was removed!\")\n",
    "    print(\"ECG index \"+str(ecg_index)+\" was removed!\")\n",
    "\n",
    "    if ica.exclude!=[]:\n",
    "        _raw = raw.copy()\n",
    "        _raw.load_data()\n",
    "        ica.apply(_raw)\n",
    "\n",
    "        fname=str(r+1)+'_components'+tail_fname+'.pdf'\n",
    "        save_multi_image(figs,os.path.join(figure_path,fname))\n",
    "        matplotlib.pyplot.close('all')\n",
    "    else:\n",
    "        _raw = raw.copy()\n",
    "        \n",
    "        fname=str(r+1)+'_components'+tail_fname+'.pdf'\n",
    "        save_multi_image(figs,os.path.join(figure_path,fname))\n",
    "        matplotlib.pyplot.close('all')\n",
    "\n",
    "    return _raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latin-practitioner",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vis_specral_density_raw(raw):\n",
    "    raw.plot_psd(fmax=50, picks=['meg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stopped-committee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vis_sensors(raw):\n",
    "    fig = plt.figure()\n",
    "    ax2d = fig.add_subplot(121)\n",
    "    ax3d = fig.add_subplot(122, projection='3d')\n",
    "    raw.plot_sensors(ch_type='grad', axes=ax2d)\n",
    "    raw.plot_sensors(ch_type='grad', axes=ax3d, kind='3d')\n",
    "    #ax3d.view_init(azim=70, elev=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electoral-payday",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vis_sensors(raw):\n",
    "    fig = mne.viz.plot_alignment(raw.info, trans=None, dig=True, eeg=False, show_axes=True,\n",
    "                                 surfaces=[], meg=['helmet', 'sensors'],\n",
    "                                 coord_frame='meg')\n",
    "\n",
    "    mne.viz.set_3d_view(fig, azimuth=50, elevation=90, distance=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "educational-virginia",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_event(raw):\n",
    "    # Sample Stimulus: 1-32\n",
    "    # Smaller Match Stimulus: 1-32 + 32\n",
    "    # Larger Match Stimulus: 1-32 + 64\n",
    "    # Button Pressed: 1,2,4,8 + 128\n",
    "    # Right Hand: Yellow (2), Green (4)\n",
    "    # Left Hand: Red (1), Blue (8)\n",
    "    # 4 comparision in each block (2 larger, 2 smaller)\n",
    "    # 16 comarision in each fif files (8 larger, 8 smaller)\n",
    "    \n",
    "    figs = []\n",
    "    \n",
    "    _raw = raw.copy()\n",
    "    events = mne.find_events(_raw, stim_channel='STI101', min_duration=.005, shortest_event=1, output='onset')\n",
    "    #events = process_diode(raw);\n",
    "\n",
    "    # Merged events for number\n",
    "    merged_events_num = events\n",
    "    merged_events_num = mne.merge_events(merged_events_num, list(range(1,9)), 1)   #N1\n",
    "    merged_events_num = mne.merge_events(merged_events_num, list(range(9,17)), 2)   #N2\n",
    "    merged_events_num = mne.merge_events(merged_events_num, list(range(17,25)), 3)   #N3\n",
    "    merged_events_num = mne.merge_events(merged_events_num, list(range(25,33)), 4)   #N4\n",
    "    merged_events_num = mne.merge_events(merged_events_num, list(range(33,65)), 5)   #Smaller Match\n",
    "    merged_events_num = mne.merge_events(merged_events_num, list(range(65,129)), 6)   #Larger Match\n",
    "    merged_events_num = mne.merge_events(merged_events_num, [129,136], 7)   #Left Button\n",
    "    merged_events_num = mne.merge_events(merged_events_num, [130,132], 8)   #Right Button\n",
    "\n",
    "    # Merged events for size\n",
    "    merged_events_size = events\n",
    "    merged_events_size = mne.merge_events(merged_events_size, [1,2,9,10,17,18,25,26], 1)   #S1\n",
    "    merged_events_size = mne.merge_events(merged_events_size, [3,4,11,12,19,20,27,28], 2)   #S2\n",
    "    merged_events_size = mne.merge_events(merged_events_size, [5,6,13,14,21,22,29,30], 3)   #S3\n",
    "    merged_events_size = mne.merge_events(merged_events_size, [7,8,15,16,23,24,31,32], 4)   #S4\n",
    "    merged_events_size = mne.merge_events(merged_events_size, list(range(33,65)), 5)   #Smaller Match\n",
    "    merged_events_size = mne.merge_events(merged_events_size, list(range(65,129)), 6)   #Larger Match\n",
    "    merged_events_size = mne.merge_events(merged_events_size, [129,136], 7)   #Left Button\n",
    "    merged_events_size = mne.merge_events(merged_events_size, [130,132], 8)   #Right Button\n",
    "\n",
    "    # Merged events for TFA\n",
    "    merged_events_tfa = events\n",
    "    merged_events_tfa = mne.merge_events(merged_events_tfa, [1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31], 1)   #TFA1\n",
    "    merged_events_tfa = mne.merge_events(merged_events_tfa, [2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32], 2)   #TFA2\n",
    "    merged_events_tfa = mne.merge_events(merged_events_tfa, list(range(33,65)), 3)   #Smaller Match\n",
    "    merged_events_tfa = mne.merge_events(merged_events_tfa, list(range(65,129)), 4)   #Larger Match\n",
    "    merged_events_tfa = mne.merge_events(merged_events_tfa, [129,136], 5)   #Left Button\n",
    "    merged_events_tfa = mne.merge_events(merged_events_tfa, [130,132], 6)   #Right Button\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    events_exp = np.delete(events, [0,1], axis=1).T\n",
    "    unique, counts = np.unique(events_exp, return_counts=True)\n",
    "    dict(zip(unique, counts))\n",
    "    keys = dict(zip(unique, counts)).keys()\n",
    "    values = dict(zip(unique, counts)).values()\n",
    "    plt.bar(keys, values)\n",
    "    for key, value in dict(zip(unique, counts)).items():\n",
    "        plt.text(key-0.65, value+0.05, str(value))\n",
    "    plt.gcf().set_size_inches(30,15)\n",
    "    figs.append(fig)\n",
    "    \n",
    "    # Number\n",
    "    figs.append(mne.viz.plot_events(merged_events_num, event_id=event_dict_N, sfreq=raw.info['sfreq'], first_samp=raw.first_samp))\n",
    "\n",
    "    # Size\n",
    "    figs.append(mne.viz.plot_events(merged_events_size, event_id=event_dict_S, sfreq=raw.info['sfreq'], first_samp=raw.first_samp))\n",
    "\n",
    "    # TFA\n",
    "    figs.append(mne.viz.plot_events(merged_events_tfa, event_id=event_dict_TFA, sfreq=raw.info['sfreq'], first_samp=raw.first_samp))\n",
    "    \n",
    "    fname = 'events'+tail_fname+'.pdf'\n",
    "    save_multi_image(figs,os.path.join(figure_path,fname))\n",
    "    matplotlib.pyplot.close('all')\n",
    "\n",
    "    return events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tight-preserve",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def epoch_data(events,raw):\n",
    "    \n",
    "    current_sfreq = raw.info['sfreq']\n",
    "    desired_sfreq = downsample_freq\n",
    "    decim = np.round(current_sfreq / desired_sfreq).astype(int)\n",
    "    \n",
    "    \"\"\"\n",
    "    # Reject criteria\n",
    "    reject_criteria = dict(mag=4000e-15,     # 4000 fT\n",
    "                           grad=4000e-13)    # 4000 fT/cm\n",
    "\n",
    "    # Flat criteria\n",
    "    flat_criteria = dict(mag=1e-15,          # 1 fT\n",
    "                         grad=1e-13)         # 1 fT/cm\n",
    "\n",
    "    # Epochs with rejecting criteria and annotation\n",
    "    epochs_sample_N = mne.Epochs(raw, events, event_id=event_sample_dict_N,\n",
    "                                 baseline=baseline, tmin=tmin, tmax=tmax, decim=decim, \n",
    "                                 reject=reject_criteria, flat=flat_criteria,\n",
    "                                 reject_by_annotation=True,\n",
    "                                 preload=True)\n",
    "\n",
    "    # Epochs with rejecting annotation\n",
    "    epochs_sample_N = mne.Epochs(raw, events, event_id=event_sample_dict_N,\n",
    "                                 baseline=baseline, tmin=tmin, tmax=tmax, decim=decim, \n",
    "                                 reject_by_annotation=True,\n",
    "                                 preload=True)\n",
    "    \"\"\"\n",
    "\n",
    "    # Epochs without rejecting criteria and annotation\n",
    "    epochs_sample_N = mne.Epochs(raw, events, event_id=event_sample_dict_N,\n",
    "                                 baseline=baseline, tmin=tmin, tmax=tmax, decim=decim)\n",
    "\n",
    "    epochs_sample_TFA = mne.Epochs(raw, events, event_id=event_sample_dict_TFA,\n",
    "                                   baseline=baseline, tmin=tmin, tmax=tmax, decim=decim)\n",
    "\n",
    "    epochs_sample_S = mne.Epochs(raw, events, event_id=event_sample_dict_S,\n",
    "                                 baseline=baseline, tmin=tmin, tmax=tmax, decim=decim)\n",
    "\n",
    "    #################### Number ####################\n",
    "    epochs_sample_N1 = epochs_sample_N['N1']\n",
    "    epochs_sample_N2 = epochs_sample_N['N2']\n",
    "    epochs_sample_N3 = epochs_sample_N['N3']\n",
    "    epochs_sample_N4 = epochs_sample_N['N4']\n",
    "\n",
    "    epochs_sample_N.drop_bad()\n",
    "    epochs_sample_N1.drop_bad()\n",
    "    epochs_sample_N2.drop_bad()\n",
    "    epochs_sample_N3.drop_bad()\n",
    "    epochs_sample_N4.drop_bad()\n",
    "\n",
    "    #################### TFA ####################\n",
    "    epochs_sample_TFA1 = epochs_sample_TFA['TFA1']\n",
    "    epochs_sample_TFA2 = epochs_sample_TFA['TFA2']\n",
    "\n",
    "    epochs_sample_TFA.drop_bad()\n",
    "    epochs_sample_TFA1.drop_bad()\n",
    "    epochs_sample_TFA2.drop_bad()\n",
    "\n",
    "    #################### Size ####################\n",
    "    epochs_sample_S1 = epochs_sample_S['S1']\n",
    "    epochs_sample_S2 = epochs_sample_S['S2']\n",
    "    epochs_sample_S3 = epochs_sample_S['S3']\n",
    "    epochs_sample_S4 = epochs_sample_S['S4']\n",
    "\n",
    "    epochs_sample_S.drop_bad()\n",
    "    epochs_sample_S1.drop_bad()\n",
    "    epochs_sample_S2.drop_bad()\n",
    "    epochs_sample_S3.drop_bad()\n",
    "    epochs_sample_S4.drop_bad()\n",
    "    \n",
    "    epochs = {\n",
    "        \"N\": epochs_sample_N,\n",
    "        \"S\": epochs_sample_S,\n",
    "        \"TFA\": epochs_sample_TFA,\n",
    "        \"N1\": epochs_sample_N1,\n",
    "        \"N2\": epochs_sample_N2,\n",
    "        \"N3\": epochs_sample_N3,\n",
    "        \"N4\": epochs_sample_N4,\n",
    "        \"S1\": epochs_sample_S1,\n",
    "        \"S2\": epochs_sample_S2,\n",
    "        \"S3\": epochs_sample_S3,\n",
    "        \"S4\": epochs_sample_S4,\n",
    "        \"TFA1\": epochs_sample_TFA1,\n",
    "        \"TFA2\": epochs_sample_TFA2}\n",
    "\n",
    "    return epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chubby-trunk",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_evoked(epochs):\n",
    "    evoked_N1 = epochs[\"N1\"].average()\n",
    "    evoked_N2 = epochs[\"N2\"].average()\n",
    "    evoked_N3 = epochs[\"N3\"].average()\n",
    "    evoked_N4 = epochs[\"N4\"].average()\n",
    "\n",
    "    evoked_TFA1 = epochs[\"TFA1\"].average()\n",
    "    evoked_TFA2 = epochs[\"TFA2\"].average()\n",
    "\n",
    "    evoked_S1 = epochs[\"S1\"].average()\n",
    "    evoked_S2 = epochs[\"S2\"].average()\n",
    "    evoked_S3 = epochs[\"S3\"].average()\n",
    "    evoked_S4 = epochs[\"S4\"].average()\n",
    "    \n",
    "    evoked = {\n",
    "        \"N1\": evoked_N1,\n",
    "        \"N2\": evoked_N2,\n",
    "        \"N3\": evoked_N3,\n",
    "        \"N4\": evoked_N4,\n",
    "        \"S1\": evoked_S1,\n",
    "        \"S2\": evoked_S2,\n",
    "        \"S3\": evoked_S3,\n",
    "        \"S4\": evoked_S4,\n",
    "        \"TFA1\": evoked_TFA1,\n",
    "        \"TFA2\": evoked_TFA2}\n",
    "    \n",
    "    return evoked\n",
    "\n",
    "def estimate_evoked_autoreject(epochs):\n",
    "    epochs_N1 = epochs['N1S1TFA1', 'N1S1TFA2', 'N1S2TFA1', 'N1S2TFA2', 'N1S3TFA1', 'N1S3TFA2', 'N1S4TFA1', 'N1S4TFA2']\n",
    "    epochs_N2 = epochs['N2S1TFA1', 'N2S1TFA2', 'N2S2TFA1', 'N2S2TFA2', 'N2S3TFA1', 'N2S3TFA2', 'N2S4TFA1', 'N2S4TFA2']\n",
    "    epochs_N3 = epochs['N3S1TFA1', 'N3S1TFA2', 'N3S2TFA1', 'N3S2TFA2', 'N3S3TFA1', 'N3S3TFA2', 'N3S4TFA1', 'N3S4TFA2']\n",
    "    epochs_N4 = epochs['N4S1TFA1', 'N4S1TFA2', 'N4S2TFA1', 'N4S2TFA2', 'N4S3TFA1', 'N4S3TFA2', 'N4S4TFA1', 'N4S4TFA2']\n",
    "    epochs_S1 = epochs['N1S1TFA1', 'N1S1TFA2', 'N2S1TFA1', 'N2S1TFA2', 'N3S1TFA1', 'N3S1TFA2', 'N4S1TFA1', 'N4S1TFA2']\n",
    "    epochs_S2 = epochs['N1S2TFA1', 'N1S2TFA2', 'N2S2TFA1', 'N2S2TFA2', 'N3S2TFA1', 'N3S2TFA2', 'N4S2TFA1', 'N4S2TFA2']\n",
    "    epochs_S3 = epochs['N1S3TFA1', 'N1S3TFA2', 'N2S3TFA1', 'N2S3TFA2', 'N3S3TFA1', 'N3S3TFA2', 'N4S3TFA1', 'N4S3TFA2']\n",
    "    epochs_S4 = epochs['N1S4TFA1', 'N1S4TFA2', 'N2S4TFA1', 'N2S4TFA2', 'N3S4TFA1', 'N3S4TFA2', 'N4S4TFA1', 'N4S4TFA2']\n",
    "    epochs_TFA1 = epochs['N1S1TFA1', 'N1S2TFA1', 'N1S3TFA1', 'N1S4TFA1', 'N2S1TFA1', 'N2S2TFA1', 'N2S3TFA1', 'N2S4TFA1',\n",
    "                         'N3S1TFA1', 'N3S2TFA1', 'N3S3TFA1', 'N3S4TFA1', 'N4S1TFA1', 'N4S2TFA1', 'N4S3TFA1', 'N4S4TFA1']\n",
    "    epochs_TFA2 = epochs['N1S1TFA2', 'N1S2TFA2', 'N1S3TFA2', 'N1S4TFA2', 'N2S1TFA2', 'N2S2TFA2', 'N2S3TFA2', 'N2S4TFA2',\n",
    "                         'N3S1TFA2', 'N3S2TFA2', 'N3S3TFA2', 'N3S4TFA2', 'N4S1TFA2', 'N4S2TFA2', 'N4S3TFA2', 'N4S4TFA2']\n",
    "\n",
    "    evoked_N1 = epochs_N1.average()\n",
    "    evoked_N2 = epochs_N2.average()\n",
    "    evoked_N3 = epochs_N3.average()\n",
    "    evoked_N4 = epochs_N4.average()\n",
    "    evoked_TFA1 = epochs_TFA1.average()\n",
    "    evoked_TFA2 = epochs_TFA2.average()\n",
    "    evoked_S1 = epochs_S1.average()\n",
    "    evoked_S2 = epochs_S2.average()\n",
    "    evoked_S3 = epochs_S3.average()\n",
    "    evoked_S4 = epochs_S4.average()\n",
    "    \n",
    "    evoked = {\n",
    "        \"N1\": evoked_N1,\n",
    "        \"N2\": evoked_N2,\n",
    "        \"N3\": evoked_N3,\n",
    "        \"N4\": evoked_N4,\n",
    "        \"S1\": evoked_S1,\n",
    "        \"S2\": evoked_S2,\n",
    "        \"S3\": evoked_S3,\n",
    "        \"S4\": evoked_S4,\n",
    "        \"TFA1\": evoked_TFA1,\n",
    "        \"TFA2\": evoked_TFA2}\n",
    "    \n",
    "    return evoked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c90efe-8027-4822-ab80-0ac7cf6e254b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vis_plot_epoch(epochs,_str):\n",
    "    #_str: \"N1\", \"N2\", \"N3\", \"N4\", \"S1\", \"S2\", \"S3\", \"S4\", \"TFA1\", \"TFA2\"\n",
    "    epochs[_str].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elementary-discretion",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vis_plot_evoked(evoked):\n",
    "    \n",
    "    figs = []\n",
    "    \n",
    "    figs = figs + mne.viz.plot_compare_evokeds(dict(N1=evoked[\"N1\"], N2=evoked[\"N2\"], N3=evoked[\"N3\"], N4=evoked[\"N4\"]),\n",
    "                                               picks= 'meg', legend='upper left', show_sensors='upper right')\n",
    "    \n",
    "    figs = figs + mne.viz.plot_compare_evokeds(dict(TFA1=evoked[\"TFA1\"], TFA2=evoked[\"TFA2\"]),\n",
    "                                               picks= 'meg', legend='upper left', show_sensors='upper right')\n",
    "\n",
    "    figs = figs + mne.viz.plot_compare_evokeds(dict(S1=evoked[\"S1\"], S2=evoked[\"S2\"], S3=evoked[\"S3\"], S4=evoked[\"S4\"]),\n",
    "                                               picks= 'meg', legend='upper left', show_sensors='upper right')\n",
    "    \n",
    "    fname='evoked'+tail_fname+'.pdf'\n",
    "    save_multi_image(figs,os.path.join(figure_path,fname))\n",
    "    matplotlib.pyplot.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "purple-reproduction",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vis_plot_joint_evoked(evoked,_str):\n",
    "    #_str: \"N1\", \"N2\", \"N3\", \"N4\", \"S1\", \"S2\", \"S3\", \"S4\", \"TFA1\", \"TFA2\"\n",
    "    evoked[_str].plot_joint(picks='mag',times=[0, 0.11, 0.17, 0.27, 0.48])\n",
    "    evoked[_str].plot_joint(picks='grad',times=[0, 0.11, 0.17, 0.27, 0.48])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72195be9-c46b-43e7-9edf-b8b27f3e29ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vis_plot_topo_evoked(evoked,_str):\n",
    "    #_str: \"N1\", \"N2\", \"N3\", \"N4\", \"S1\", \"S2\", \"S3\", \"S4\", \"TFA1\", \"TFA2\"\n",
    "    evoked[_str].pick_types(meg='mag').plot_topo(color='r', legend=False)\n",
    "    evoked[_str].pick_types(meg='grad').plot_topo(color='r', legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sophisticated-tuesday",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vis_map_topo_evoked(evoked,_str):\n",
    "    evoked[_str].plot_topomap(times=[0, 0.11, 0.17, 0.27, 0.48], ch_type='mag')\n",
    "    evoked[_str].plot_topomap(times=[0, 0.11, 0.17, 0.27, 0.48], ch_type='grad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8db3022-b8fc-4be2-b418-08b5297ae773",
   "metadata": {},
   "outputs": [],
   "source": [
    "def autoreject_detect_bad_sensors(raw):\n",
    "    \n",
    "    current_sfreq = raw.info['sfreq']\n",
    "    desired_sfreq = downsample_freq\n",
    "    decim = np.round(current_sfreq / desired_sfreq).astype(int)\n",
    "    \n",
    "    _raw = raw.copy()\n",
    "    _event = mne.find_events(_raw, stim_channel='STI101', min_duration=.005, shortest_event=1, output='onset')\n",
    "\n",
    "    # Autoreject can repair only one channel type at a time.\n",
    "    picks_grad = mne.pick_types(_raw.info, meg='grad', eeg=False, stim=False, eog=False, ecg=False, include=[], exclude=[])\n",
    "    picks_mag = mne.pick_types(_raw.info, meg='mag', eeg=False, stim=False, eog=False, ecg=False, include=[], exclude=[])\n",
    "\n",
    "    # Remove proj, don't proj while interpolating\n",
    "    _raw.del_proj()\n",
    "    _epochs = mne.Epochs(_raw, _event, event_id=event_sample_dict, baseline=baseline, tmin=tmin, tmax=tmax, decim=decim, preload=True)\n",
    "\n",
    "    ransac = Ransac(verbose='progressbar', picks=picks_grad, n_jobs=1, min_corr=0.5)\n",
    "    ransac.fit(_epochs)\n",
    "    print(\"Gradiometer:\")\n",
    "    print('\\n'.join(ransac.bad_chs_))\n",
    "\n",
    "    ransac = Ransac(verbose='progressbar', picks=picks_mag, n_jobs=1, min_corr=0.5)\n",
    "    ransac.fit(_epochs)\n",
    "    print(\"Magnetometer:\")\n",
    "    print('\\n'.join(ransac.bad_chs_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678366ac-3c10-4fed-8f55-d0e76640ca40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_mat(epochs):\n",
    "    _epochs = copy.deepcopy(epochs)\n",
    "    _epochs.load_data().pick_types(meg=True, eeg=False, stim=False, eog=False, ecg=False)\n",
    "\n",
    "    _epochs.drop_bad()\n",
    "    _epochs.equalize_event_counts(event_sample_list)\n",
    "\n",
    "    #_epochs.save('mne_python-epo.fif')\n",
    "    #mne.write_events('mne_python-eve.fif', _epochs.events)\n",
    "\n",
    "    unique, counts = np.unique(_epochs.events[:,2], return_counts=True)\n",
    "    dict(zip(unique, counts))\n",
    "    keys = dict(zip(unique, counts)).keys()\n",
    "    values = dict(zip(unique, counts)).values()\n",
    "    print(str(len(unique))+\" epoches have been extracted!\")\n",
    "    for key, value in dict(zip(unique, counts)).items():\n",
    "        print(\"There are \"+str(value)+\" events in the epoch \"+str(key))\n",
    "\n",
    "    data = _epochs.get_data()\n",
    "\n",
    "    # Reduce dimension using PCA\n",
    "    #pca = UnsupervisedSpatialFilter(PCA(30), average=True)\n",
    "    #data = pca.fit_transform(data)\n",
    "\n",
    "    stim_index = np.zeros(len(_epochs.events), dtype=int)\n",
    "    stim_label = list()\n",
    "    for i in range(len(_epochs.events)):\n",
    "        stim_index[i] = _epochs.events[i][2]\n",
    "        stim_label.append(list(event_sample_dict.keys())[list(event_sample_dict.values()).index(_epochs.events[i][2])])\n",
    "\n",
    "    epochs_dict = {\n",
    "        # n_samples * n_channels * n_time\n",
    "        \"trial\": data,\n",
    "        \"index\": stim_index,\n",
    "        \"label\": stim_label,\n",
    "        \"time\": np.linspace(1000*tmin,1000*tmax,data.shape[2]),\n",
    "        \"channel_name\": _epochs.ch_names,\n",
    "        \"channel_type\": _epochs.get_channel_types(picks='meg')}\n",
    "\n",
    "    fname = epoched_fname\n",
    "    scipy.io.savemat(os.path.join(epoched_path,fname),epochs_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1c3beb-86ab-4150-bf96-30180bae2be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Import Raw Data ##########\n",
    "prepare_file(rename_file=True)\n",
    "raws = import_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd2a50b-e50e-411a-ae6f-f7c55551c5ba",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "########## Inspect Raw Data ##########\n",
    "run = 1\n",
    "handle_bad_channels(raws[run-1])\n",
    "vis_plot_raw(raws[run-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d180b049-1965-40cc-9cea-ebf71fa1e263",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture cap --no-stderr\n",
    "\n",
    "########## Import Data and Information ########## \n",
    "prepare_file(rename_file=False)\n",
    "raws = import_data()\n",
    "subject_info = import_info()\n",
    "modify_raw_obj(raws,subject_info)\n",
    "\n",
    "for r in range(n_runs):\n",
    "    ########## Print Information ##########\n",
    "    print(\"************************* Data Information - run: \"+str(r+1)+\" *************************\")\n",
    "    print_info(raws[r])\n",
    "    \n",
    "    ########## Filter Data ########## \n",
    "    print(\"************************* Power-line noise filter - run: \"+str(r+1)+\" *************************\")\n",
    "    raws[r] = filter_power_noise(raws[r],r)\n",
    "\n",
    "    print(\"************************* Band-pass filter - run: \"+str(r+1)+\" *************************\")\n",
    "    raws[r] = filter_band(raws[r],r)\n",
    "\n",
    "    ########## Run ICA ########## \n",
    "    if \"ica\" in tail_fname:\n",
    "        print(\"************************* Run ICA - run: \"+str(r+1)+\" *************************\")\n",
    "        raws[r] = fit_ica(raws[r],r)\n",
    "\n",
    "    ########## Annotate Muscle ########## \n",
    "    print(\"************************* Annotate muscle activity - run: \"+str(r+1)+\" *************************\")\n",
    "    raws[r] = gen_annot_muscle(raws[r])\n",
    "\n",
    "    ########## Smooth Data ########## \n",
    "    print(\"************************* Smooth data - run: \"+str(r+1)+\" *************************\")\n",
    "    raws[r] = smooth_savgol_raw(raws[r])\n",
    "    #raws[r] = smooth_butter_raw(raws[r])\n",
    "\n",
    "    ########## Downsample Data ########## \n",
    "    print(\"************************* Downsample data - run: \"+str(r+1)+\" *************************\")\n",
    "    raws[r] = downsample_raw(raws[r])\n",
    "    \n",
    "    ########## Pick MEG ##########\n",
    "    raws[r].pick_types(meg=True, eeg=False, stim=True, misc=True, eog=False, ecg=False, include=[], exclude=[])\n",
    "\n",
    "########## Concatenate Data ##########\n",
    "print(\"************************* Concatenating raws *************************\")\n",
    "raw = concatenate_raw_obj(raws,on_mismatch='warn')\n",
    "\n",
    "########## Create Events, Epochs, and Evoked Data then Save Epochs ##########\n",
    "events = make_event(raw)\n",
    "epochs = epoch_data(events,raw)\n",
    "evoked = estimate_evoked(epochs)\n",
    "vis_plot_evoked(evoked)\n",
    "export_mat(epochs[\"N\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff36776f-d652-4de8-81f7-a3599eb8bbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Write Log ##########\n",
    "fname = log_fname\n",
    "with open(os.path.join(log_path,fname), 'w') as f:\n",
    "    f.write(cap.stdout)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 862.85,
   "position": {
    "height": "786.85px",
    "left": "1374px",
    "right": "20px",
    "top": "130px",
    "width": "498px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
